{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d9b11772074d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/yonathan/Documents/projects/faceSwap\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfaceSwap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/projects/faceSwap/faceSwap.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0mfaceSwap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaceSwap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m \u001b[0mfaceSwap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfaceSwap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshowOriginalImages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/projects/faceSwap/faceSwap.py\u001b[0m in \u001b[0;36mfaceSwap\u001b[0;34m(self, showOriginalImages)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;31m# find landmark points of larger face in an images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mlandmark_points1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_larger_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces_landmark_points1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0mlandmark_points2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_larger_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces_landmark_points2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/faceSwap/faceSwap.py\u001b[0m in \u001b[0;36mchoose_larger_face\u001b[0;34m(self, faces_landmark_points)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces_landmark_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mboundingRect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboundingRect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces_landmark_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/yonathan/Documents/projects/faceSwap\")\n",
    "import faceSwap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def landmark_detection(image, returnImage=False):\n",
    "        \"\"\"Generate facial landmark points of a give image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : str\n",
    "            image file path.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        landmark_points : list\n",
    "            return a list of tuple integer of the landmark points.\n",
    "        img : numpy array\n",
    "            return an image where the landmark points draw on the the image.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # laod an image then convert it to grey scale\n",
    "        img = cv2.imread(image)\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # detect the face then find the landmarks\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        predictor = dlib.shape_predictor(\n",
    "            'shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "        faces = detector(img_gray)\n",
    "        faces_landmark_points = []\n",
    "        for face in faces:\n",
    "            landmarks = predictor(img_gray, face)\n",
    "            landmark_points = []\n",
    "            for n in range(68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                landmark_points.append((int(x), int(y)))\n",
    "\n",
    "                cv2.circle(img, (x, y), 3, (0, 0, 255), -1)\n",
    "\n",
    "            faces_landmark_points.append(landmark_points)\n",
    "\n",
    "        if returnImage:\n",
    "            return faces_landmark_points, img\n",
    "        else:\n",
    "            return faces_landmark_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"/home/yonathan/Documents/projects/faceSwap/images/original_images/anchorman.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_landmark_points = landmark_detection(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = np.array(faces_landmark_points).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_largest_face(self, faces_landmark_points, img):\n",
    "\n",
    "    size = 0\n",
    "    faces = np.array(faces_landmark_points).shape[0]\n",
    "    for face in range(faces):\n",
    "        boundingRect = cv2.boundingRect(\n",
    "            np.array(faces_landmark_points[face]))\n",
    "        face_size = (boundingRect[2] - boundingRect[0]) * \\\n",
    "            (boundingRect[3] - boundingRect[1])\n",
    "        cv2.rectangle(img, (boundingRect[0:2]), boundingRect[2:4], (255, 0,0))\n",
    "        if face_size > size:\n",
    "            size = face_size\n",
    "            larger_face_index = face\n",
    "\n",
    "    largest_face_landmark_points = faces_landmark_points[larger_face_index]\n",
    "    return largest_face_landmark_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmark_points = faces_landmark_points[0]\n",
    "np.array(landmark_points).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 269, 321, 308)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundingRect = cv2.boundingRect(np.array(landmark_points))\n",
    "boundingRect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6825"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = (boundingRect[2] - boundingRect[0]) * (boundingRect[3] - boundingRect[1])\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(64, 436),\n",
       " (67, 450),\n",
       " (71, 466),\n",
       " (77, 481),\n",
       " (85, 496),\n",
       " (94, 509),\n",
       " (103, 523),\n",
       " (112, 536),\n",
       " (124, 541),\n",
       " (140, 539),\n",
       " (156, 528),\n",
       " (170, 513),\n",
       " (183, 495),\n",
       " (191, 475),\n",
       " (195, 454),\n",
       " (197, 431),\n",
       " (196, 408),\n",
       " (58, 418),\n",
       " (63, 414),\n",
       " (70, 415),\n",
       " (78, 417),\n",
       " (85, 420),\n",
       " (106, 417),\n",
       " (118, 410),\n",
       " (132, 405),\n",
       " (147, 405),\n",
       " (161, 411),\n",
       " (99, 432),\n",
       " (99, 446),\n",
       " (98, 460),\n",
       " (98, 473),\n",
       " (94, 478),\n",
       " (99, 481),\n",
       " (105, 482),\n",
       " (112, 480),\n",
       " (119, 477),\n",
       " (72, 431),\n",
       " (76, 427),\n",
       " (84, 427),\n",
       " (91, 432),\n",
       " (84, 435),\n",
       " (76, 435),\n",
       " (123, 430),\n",
       " (128, 423),\n",
       " (138, 422),\n",
       " (147, 425),\n",
       " (140, 431),\n",
       " (131, 433),\n",
       " (103, 497),\n",
       " (104, 495),\n",
       " (106, 494),\n",
       " (111, 496),\n",
       " (116, 494),\n",
       " (127, 495),\n",
       " (140, 496),\n",
       " (130, 503),\n",
       " (120, 506),\n",
       " (114, 507),\n",
       " (109, 506),\n",
       " (106, 502),\n",
       " (106, 497),\n",
       " (108, 497),\n",
       " (112, 498),\n",
       " (118, 498),\n",
       " (136, 496),\n",
       " (118, 499),\n",
       " (112, 500),\n",
       " (108, 498)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(image)\n",
    "index = choose_largest_face(faces_landmark_points, img)\n",
    "cv2.imshow(\"Image\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
